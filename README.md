# Agentic AI Internship: Multi-User Patient Chatbot

A sophisticated multi-user AI chatbot application that simulates patient behavior for medical consultation training. Built as part of the Agentic AI Internship evaluation task, this application enables doctors to interact with an AI-powered patient that progressively reveals symptoms and evaluates treatment recommendations.

## üéØ Project Overview

This project implements a full-stack AI chatbot system where:
- **AI Patient**: Simulates a patient with realistic behavior patterns
- **Doctors (Users)**: Interact with the patient through a web interface
- **Multi-User Support**: Multiple doctors can interact simultaneously with independent sessions
- **Progressive Interaction**: Patient reveals symptoms progressively based on conversation flow
- **Treatment Evaluation**: Patient evaluates and accepts/rejects prescribed treatments

## ‚ú® Key Features

- ü§ñ **LangGraph State Machine**: 4-node conversational flow (initial ‚Üí questioning ‚Üí progressive ‚Üí treatment)
- üìà **Progressive Symptom Description**: Patient reveals symptoms gradually as conversation deepens
- üíä **Treatment Acceptance Logic**: Patient intelligently evaluates prescribed treatments
- üë• **Multi-User Capability**: Independent sessions for each user with complete isolation
- üé® **Modern UI**: Clean, responsive React interface with real-time chat
- üîí **Session Isolation**: Complete data isolation between different user sessions
- üöÄ **Production Ready**: Deployed on Vercel with proper error handling

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  React Frontend ‚îÇ  (User Interface)
‚îÇ   (Vite + JSX)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ HTTP/REST
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Flask Backend  ‚îÇ  (REST API)
‚îÇ   (Python 3.11) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   LangGraph     ‚îÇ  (State Management)
‚îÇ  State Machine  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Google Gemini  ‚îÇ  (AI Model)
‚îÇ      API        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technology Stack

**Frontend:**
- React 19.2.0
- Vite 7.2.4
- Modern JavaScript (ES6+)
- CSS3 with responsive design

**Backend:**
- Flask 2.3.2 (Python web framework)
- LangChain 1.x (AI orchestration)
- LangGraph 1.x (State machine management)
- Google Generative AI SDK (Gemini API)
- Flask-CORS (Cross-origin support)

**Deployment:**
- Vercel (Serverless hosting)
- Gunicorn (WSGI server)

## üìÅ Project Structure

```
rockfrog_chatbot/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                    # Flask API server & endpoints
‚îÇ   ‚îú‚îÄ‚îÄ agent_logic_langgraph.py  # LangGraph state machine implementation
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ vercel.json              # Vercel deployment configuration
‚îÇ   ‚îî‚îÄ‚îÄ Procfile                 # Process configuration for Vercel
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx              # Main React application component
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.jsx             # React entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Chat.jsx         # Chat interface component
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Chat.css         # Chat styling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sessionservice.js    # API service layer
‚îÇ   ‚îú‚îÄ‚îÄ package.json             # Node.js dependencies
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.js           # Vite configuration
‚îÇ
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üöÄ Quick Start

### Prerequisites

- **Python 3.11+** (for backend)
- **Node.js 18+** (for frontend)
- **Google Gemini API Key** ([Get one here](https://makersuite.google.com/app/apikey))

### Local Development Setup

#### 1. Backend Setup

```bash
# Navigate to backend directory
cd backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create .env file with your API key
echo GEMINI_API_KEY=your_api_key_here > .env

# Run the Flask server
python app.py
```

The backend will start on `http://localhost:8000`

#### 2. Frontend Setup

```bash
# Navigate to frontend directory (in a new terminal)
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

The frontend will start on `http://localhost:5173`

### Testing the Application

1. **Open the application** in your browser at `http://localhost:5173`
2. **Start a conversation** by sending "Hello" or "Hi"
3. **Observe progressive symptoms**: Ask follow-up questions to see the patient reveal more details
4. **Prescribe treatment**: Try "I prescribe you [medication name]"
5. **Test multi-user**: Open the app in multiple browser windows/tabs to see independent sessions

## üì° API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/session` | Create a new chat session (returns `session_id`) |
| `POST` | `/api/message` | Send a message to the patient (requires `session_id` and `message`) |
| `GET` | `/api/logs/<session_id>` | Retrieve conversation logs for a session |
| `GET` | `/api/health` | Health check endpoint |

### Example API Usage

**Create Session:**
```bash
curl -X POST http://localhost:8000/api/session
# Response: {"session_id": "uuid-here"}
```

**Send Message:**
```bash
curl -X POST http://localhost:8000/api/message \
  -H "Content-Type: application/json" \
  -d '{"session_id": "uuid-here", "message": "Hello, how are you feeling?"}'
# Response: {"reply": "Patient's response..."}
```

## üöÄ Deployment on Vercel

### Backend Deployment

1. **Install Vercel CLI** (if not already installed):
   ```bash
   npm install -g vercel
   ```

2. **Deploy backend**:
   ```bash
   cd backend
   vercel --prod
   ```

3. **Set environment variable** in Vercel dashboard:
   - Go to your project settings ‚Üí Environment Variables
   - Add `GEMINI_API_KEY` with your API key value

### Frontend Deployment

1. **Set backend URL**:
   - Create `.env.production` file in frontend directory:
     ```
     VITE_API_URL=https://your-backend-url.vercel.app
     ```

2. **Deploy frontend**:
   ```bash
   cd frontend
   vercel --prod
   ```

3. **Update environment variable** in Vercel dashboard:
   - Add `VITE_API_URL` pointing to your deployed backend URL

## üß† How It Works

### LangGraph State Machine

The chatbot uses a 4-state LangGraph implementation:

1. **Initial State**: Patient greets and introduces basic symptoms
2. **Questioning State**: Patient responds to doctor's questions
3. **Progressive State**: Patient reveals more detailed symptoms as conversation progresses
4. **Treatment State**: Patient evaluates and responds to treatment prescriptions

### State Transitions

- Conversation flow is managed by LangGraph's state machine
- Each state has specific behavior patterns
- Transitions occur based on conversation context and keywords
- Memory is maintained per session using LangChain's memory system

### Multi-User Support

- Each user gets a unique `session_id` when creating a session
- Sessions are completely isolated in memory
- No data leakage between different users
- Each session maintains its own conversation history

## üõ†Ô∏è Technologies & Design Choices

### Why LangGraph?
- **State Management**: Provides explicit state machine for complex conversational flows
- **Modularity**: Easy to add/remove states and transitions
- **Debugging**: Clear visualization of conversation flow
- **Scalability**: Can handle complex multi-step interactions

### Why Flask?
- **Lightweight**: Minimal overhead for API endpoints
- **Flexibility**: Easy to integrate with LangChain/LangGraph
- **Vercel Support**: Native support for Python/Flask deployments
- **Simplicity**: Straightforward REST API implementation

### Why React + Vite?
- **Modern Development**: Fast hot module replacement
- **Performance**: Optimized production builds
- **User Experience**: Smooth, responsive interface
- **Maintainability**: Component-based architecture

## üìä Requirements Compliance

‚úÖ **LangChain Integration** - Full LangChain integration for AI orchestration  
‚úÖ **LangGraph State Machine** - 4-node state machine for conversation flow  
‚úÖ **Flask Backend API** - RESTful API with proper error handling  
‚úÖ **React Frontend** - Modern, responsive user interface  
‚úÖ **Patient Simulation** - Realistic patient behavior patterns  
‚úÖ **Progressive Symptoms** - Symptoms revealed based on conversation depth  
‚úÖ **Treatment Acceptance** - Patient evaluates and accepts treatments  
‚úÖ **Multi-User Support** - Independent sessions for each user  
‚úÖ **Session Isolation** - Complete data isolation between sessions  
‚úÖ **Vercel Deployment** - Both frontend and backend deployed on Vercel  
‚úÖ **Minimal Logging** - Essential logging without verbosity  

## üêõ Troubleshooting

### Common Issues

**API Rate Limiting (429 errors):**
- Google Gemini free tier has rate limits (typically 5 requests/minute)
- The code includes retry logic with exponential backoff
- Wait 1 minute between requests if you hit the limit
- Consider upgrading to a paid tier for production use

**Frontend can't connect to backend:**
- Verify `VITE_API_URL` environment variable is set correctly
- Check that backend is running and accessible
- Ensure CORS is enabled (already configured in `app.py`)
- Check browser console for specific error messages

**Session not persisting:**
- Sessions are stored in-memory (for development)
- Each server restart clears all sessions
- For production, consider using Redis or a database

**Import errors:**
- Ensure all dependencies are installed: `pip install -r requirements.txt`
- Verify Python version is 3.11+
- Check that virtual environment is activated

## üìù Environment Variables

### Backend
- `GEMINI_API_KEY` (Required) - Your Google Gemini API key
- `PORT` (Optional) - Server port (default: 8000)

### Frontend
- `VITE_API_URL` (Required for production) - Backend API URL
  - Development: `http://localhost:8000`
  - Production: Your deployed backend URL

## üìö Additional Resources

- [LangChain Documentation](https://python.langchain.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Google Gemini API](https://ai.google.dev/)
- [Vercel Deployment Guide](https://vercel.com/docs)
- [Flask Documentation](https://flask.palletsprojects.com/)

## üéì Project Deliverables

This project was developed as part of the Agentic AI Internship evaluation task and includes:

- ‚úÖ **Working Application**: Fully functional chatbot deployed on Vercel
- ‚úÖ **Complete Source Code**: All code uploaded to repository
- ‚úÖ **Architecture Documentation**: This README covers architecture and design choices
- ‚úÖ **Technology Stack**: LangChain, LangGraph, Flask, React, Vercel
- ‚úÖ **Multi-User Demonstration**: Supports multiple concurrent users

## üìÑ License

This project is created for the Agentic AI Internship evaluation task.

---

**Built with ‚ù§Ô∏è using LangChain, LangGraph, Flask, and React**
